import streamlit as st
import os
from langchain_ibm import WatsonxLLM
from ibm_watsonx_ai import APIClient, Credentials
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Set API key and project ID
watsonx_api_key = os.getenv("API_KEY", None)
project_id = os.getenv("PROJECT_ID", None)
wx_url = os.getenv("WX_URL", None)

# Check if API Key and Project ID exist
if not all([watsonx_api_key, project_id, wx_url]):
    st.error("Please set the API_KEY, PROJECT_ID, and WX_URL in the environment variables.")
    st.stop()

# Initialize API Client
credentials = Credentials(url=wx_url, api_key=watsonx_api_key)
api_client = APIClient(credentials, project_id=project_id)

# Task Options
task_options = {
    "text-generation": "æ–‡æœ¬ç”Ÿæˆ",
    "summarization": "æ‘˜è¦",
    "code-generation": "ç¨‹å¼ç¢¼ç”Ÿæˆ",
    "translation": "ç¿»è­¯",
}

# Model Options
model_options = {
    "meta-llama/llama-3-1-70b-instruct": "Llama 3-1 70B Instruct",
    "mistralai/mistral-large": "Mistral Large",
    "meta-llama/llama-3-1-8b-instruct": "LLAMA 3-1 8B INSTRUCT",
    "ibm/granite-3-8b-instruct": "Granite 3 8B Instruct",
    "ibm/granite-3-2b-instruct": "Granite 3 2B Instruct",
    "meta-llama/llama-3-405b-instruct": "Llama 3 405B Instruct",
}

# Task-specific prompt templates
task_prompts = {
    "text-generation": (
        "è«‹ç”¨å°ç£ç¹é«”ä¸­æ–‡å›è¦†æ‰€æœ‰å…§å®¹ã€‚"
        "è«‹æ ¹æ“šä»¥ä¸‹æä¾›çš„æ–‡æœ¬ï¼Œå‰µå»ºä¸€æ®µé€£è²«ä¸”æœ‰é‚è¼¯çš„å¾ŒçºŒå…§å®¹ã€‚"
        "è«‹ç¢ºä¿ç”Ÿæˆçš„å…§å®¹ç¬¦åˆä¸Šä¸‹æ–‡ï¼Œä¸¦ä¿æŒèªæ°£å’Œé¢¨æ ¼çš„ä¸€è‡´ã€‚"
        "è¼¸å…¥çš„æ–‡æœ¬æ˜¯ï¼š\n{input_text}\n"
        "è«‹æ³¨æ„ç”Ÿæˆçš„æ–‡æœ¬æ‡‰è©²æµæš¢ä¸”è‡ªç„¶ï¼Œé¿å…é‡è¤‡å’Œä¸ç›¸é—œçš„ä¿¡æ¯ã€‚"
    ),
    "summarization": (
        "è«‹ç”¨å°ç£ç¹é«”ä¸­æ–‡å›è¦†æ‰€æœ‰å…§å®¹ã€‚"
        "è«‹é–±è®€ä»¥ä¸‹æ–‡æœ¬ï¼Œä¸¦æ ¹æ“šå…¶å…§å®¹ç”Ÿæˆä¸€å€‹ç°¡æ˜æ‰¼è¦çš„æ‘˜è¦ã€‚"
        "æ‘˜è¦æ‡‰è©²æ¶µè“‹ä¸»è¦çš„è§€é»å’Œé‡è¦è³‡è¨Šï¼Œä¸¦ä¿æŒåŸæ„ã€‚"
        "è¼¸å…¥çš„æ–‡æœ¬æ˜¯ï¼š\n{input_text}\n"
        "è«‹ç¢ºä¿æ‘˜è¦ç°¡æ½”æ˜äº†ï¼Œä¸è¶…éæŒ‡å®šå­—æ•¸ï¼Œä¸”ä¸åŒ…å«ä¸ç›¸é—œçš„ç´°ç¯€ã€‚"
    ),
    "code-generation": (
        "ä½ æ˜¯ä¸€å€‹å¯«ç¨‹å¼çš„é«˜æ‰‹ï¼Œè«‹æ ¹æ“šä»¥ä¸‹æè¿°ï¼Œç·¨å¯«ç›¸æ‡‰çš„ç¨‹å¼ç¢¼ã€‚"
        "ç¨‹å¼ç¢¼æ‡‰è©²å…·æœ‰è‰¯å¥½çš„çµæ§‹å’Œè¨»é‡‹ï¼Œä¸¦æŒ‰ç…§æœ€ä½³å¯¦è¸é€²è¡Œç·¨å¯«ã€‚"
        "è«‹ç¢ºä¿ç¨‹å¼ç¢¼èƒ½å¤ åŸ·è¡Œä¸¦é”åˆ°é æœŸåŠŸèƒ½ã€‚"
        "æè¿°å¦‚ä¸‹ï¼š\n{input_text}\n"
        "è«‹æ³¨æ„ï¼Œè¼¸å‡ºçš„ç¨‹å¼ç¢¼æ‡‰è©²ç²¾ç°¡æ˜ç¢ºï¼Œä¸¦åŒ…å«å¿…è¦çš„è¨»é‡‹ä¾†è§£é‡‹å…¶é‚è¼¯ã€‚"
        "ç¨‹å¼ç¢¼ç”Ÿæˆå®Œæˆå¾Œå°±è«‹åœæ­¢ç”Ÿæˆå…§å®¹ï¼Œä¸è¦ç”Ÿæˆä¸å¿…è¦çš„å…§å®¹ã€‚"
    ),
    "translation": (
        "ä½ æ˜¯ä¸€å€‹ç¿»è­¯é«˜æ‰‹ï¼Œè«‹å°‡ä»¥ä¸‹æä¾›çš„æ–‡æœ¬ç¿»è­¯ç‚ºæµæš¢çš„å°ç£ç¹é«”ä¸­æ–‡ã€‚"
        "è«‹ç¢ºä¿ç¿»è­¯ä¿ç•™åŸå§‹æ–‡æœ¬çš„å«ç¾©å’Œèªæ°£ï¼Œä¸¦ä¸”ç¬¦åˆç¹é«”ä¸­æ–‡çš„èªæ³•è¦ç¯„ã€‚"
        "è¼¸å…¥çš„æ–‡æœ¬æ˜¯ï¼š\n{input_text}\n"
        "è«‹æ³¨æ„ï¼Œç¿»è­¯æ‡‰é¿å…é€å­—ç¿»è­¯ï¼Œæ‡‰è©²æ ¹æ“šä¸Šä¸‹æ–‡ä¾†é€²è¡Œæµæš¢è‡ªç„¶çš„è½‰æ›ã€‚"
        "ç¿»è­¯å®Œæˆå¾Œå°±è«‹åœæ­¢ç”Ÿæˆå…§å®¹ï¼Œä¸è¦ç”Ÿæˆä¸å¿…è¦çš„å…§å®¹ã€‚"
    ),
}

# Default Parameters
default_parameters = {
    "decoding_method": "greedy",
    "max_new_tokens": 1000,
    "min_new_tokens": 30
    # "stop_sequences": "\n\n"
}

# Custom Styles
st.markdown("""
    <style>
        .title { font-size: 35px; color: #4B8BBE; text-align: center; font-weight: bold; margin-bottom: 20px; }
        .header { font-size: 20px; color: #306998; font-weight: bold; margin-top: 20px; }
        .metric { font-size: 18px; color: #FF6347; font-weight: bold; margin-top: 10px; padding: 10px; background-color: #f4f4f4; border-radius: 10px; text-align: center; border: 1px solid #ddd; }
        .upload-box { background-color: #f0f0f0; padding: 20px; border-radius: 10px; margin-top: 20px; }
        .container { max-width: 1800px; margin: 0 auto; }
        .output-box { border: 2px solid #4B8BBE; padding: 15px; border-radius: 10px; background-color: #f9f9f9; margin-top: 20px; color: #000000; }
        .model-header { background-color: #4B8BBE; color: #ffffff; padding: 10px; border-radius: 5px; text-align: center; font-weight: bold; }
    </style>
""", unsafe_allow_html=True)

# Streamlit UI Layout
st.markdown('<div class="container">', unsafe_allow_html=True)
st.markdown('<div class="title">ğŸ¤– LLM Model Comparison Arena ğŸ¤–</div>', unsafe_allow_html=True)
st.sidebar.title("Model & Task Selection")


# Task selection
task_type = st.sidebar.selectbox("Select Task", list(task_options.keys()), format_func=lambda x: task_options[x])
st.sidebar.markdown(f"**Task Type:** {task_options[task_type]}")


# Model 1 selection
model_1_id = "ibm/granite-3-8b-instruct"

# Parameters for Model 1
st.sidebar.markdown(f"### {model_options[model_1_id]} Parameters")
decoding_method_1 = st.sidebar.radio(f"Select Decoding Method for {model_options[model_1_id]}", ["greedy","sample"], key="decoding_method_1")
min_tokens_1 = st.sidebar.number_input(f"Min Tokens for {model_options[model_1_id]}", min_value=1, max_value=500, value=1, key="min_tokens_1")
max_tokens_1 = st.sidebar.number_input(f"Max Tokens for {model_options[model_1_id]}", min_value=10, max_value=2000, value=500, key="max_tokens_1")
temperature_1 = st.sidebar.slider(f"Temperature for {model_options[model_1_id]}", min_value=0.0, max_value=1.0, value=0.7, key="temperature_1") if decoding_method_1 == "sample" else None
top_k_1 = st.sidebar.slider(f"Top-K for {model_options[model_1_id]}", min_value=1, max_value=100, value=50, key="top_k_1") if decoding_method_1 == "sample" else None
top_p_1 = st.sidebar.slider(f"Top-P for {model_options[model_1_id]}", min_value=0.0, max_value=1.0, value=1.0, key="top_p_1") if decoding_method_1 == "sample" else None


# Model 2 selection
model_2_id = st.sidebar.selectbox("Select Model 2", list(model_options.keys()), format_func=lambda x: model_options[x])

# Parameters for Model 2
# st.sidebar.markdown(f"### {model_options[model_2_id]} Parameters")
# decoding_method_2 = st.sidebar.radio(f"Select Decoding Method for {model_options[model_2_id]}", ["sample", "greedy"], key="decoding_method_2")
# min_tokens_2 = st.sidebar.number_input(f"Min Tokens for {model_options[model_2_id]}", min_value=1, max_value=500, value=1, key="min_tokens_2")
# max_tokens_2 = st.sidebar.number_input(f"Max Tokens for {model_options[model_2_id]}", min_value=10, max_value=2000, value=1000, key="max_tokens_2")
# temperature_2 = st.sidebar.slider(f"Temperature for {model_options[model_2_id]}", min_value=0.0, max_value=1.0, value=0.7, key="temperature_2") if decoding_method_2 == "sample" else None
# top_k_2 = st.sidebar.slider(f"Top-K for {model_options[model_2_id]}", min_value=1, max_value=100, value=50, key="top_k_2") if decoding_method_2 == "sample" else None
# top_p_2 = st.sidebar.slider(f"Top-P for {model_options[model_2_id]}", min_value=0.0, max_value=1.0, value=1.0, key="top_p_2") if decoding_method_2 == "sample" else None

# User Input Text
input_text = st.text_area("Input Text", placeholder="Enter the text you want to process...", height=150)

# Submit Button
if st.button("Submit Task"):
    # Create model instances with user-defined parameters
    parameters_1 = {
        "decoding_method": decoding_method_1,
        "max_new_tokens": max_tokens_1,
        "min_new_tokens": min_tokens_1,
        # "stop_sequences": "\n\n"
        # "temperature": temperature_1,
        # "top_k": top_k_1,
        # "top_p": top_p_1,
    }

    parameters_2 = {
        "decoding_method": decoding_method_1,
        "max_new_tokens": max_tokens_1,
        "min_new_tokens": min_tokens_1,
        # "temperature": temperature_1,
        # "top_k": top_k_1,
        # "top_p": top_p_1,
    }

    model_1 = WatsonxLLM(
        model_id=model_1_id,
        watsonx_client=api_client,
        params=parameters_1
    )

    model_2 = WatsonxLLM(
        model_id=model_2_id,
        watsonx_client=api_client,
        params=parameters_2
    )

    model_3 = WatsonxLLM(
        model_id="meta-llama/llama-3-405b-instruct",  # Fixed evaluation model
        watsonx_client=api_client,
        params=default_parameters
    )

    # Generate prompt for selected task type
    input_prompt = task_prompts[task_type].format(input_text=input_text)
    st.warning(input_prompt)
    prompt_list = [input_prompt]  # Wrap input text as list

    # Model 1 and Model 2 outputs
    col1, col2 = st.columns(2)
    with col1:
        st.markdown(f'<div class="model-header">{model_options[model_1_id]}</div>', unsafe_allow_html=True)
        model_1_output_placeholder = st.empty()  # Create placeholder for streaming output
        model_1_text = ""
        for chunk in model_1.stream(input=prompt_list):
            model_1_text += chunk  # Accumulate chunk text
            model_1_output_placeholder.markdown(f'<div class="output-box">{model_1_text}</div>', unsafe_allow_html=True)

    with col2:
        st.markdown(f'<div class="model-header">{model_options[model_2_id]}</div>', unsafe_allow_html=True)
        model_2_output_placeholder = st.empty()  # Create placeholder for streaming output
        model_2_text = ""
        for chunk in model_2.stream(input=prompt_list):
            model_2_text += chunk  # Accumulate chunk text
            model_2_output_placeholder.markdown(f'<div class="output-box">{model_2_text}</div>', unsafe_allow_html=True)

    # Evaluation by Model 3
    st.markdown('<div class="model-header">Evaluation by Model 3 (Llama 3 405B Instruct)</div>', unsafe_allow_html=True)
    evaluation_task = f"è«‹ç”¨å°ç£ç”¨èªçš„ç¹é«”ä¸­æ–‡å›ç­”ï¼Œä¸¦åˆ†åˆ¥çµ¦æ¨¡å‹1ç›’æ¨¡å‹2æ¨¡å‹è¼¸å‡ºçµæœé€²è¡Œåˆ†æ•¸ 0~10åˆ†ã€‚ è«‹è©•ä¼°æ­¤å…©å€‹æ¨¡å‹æ‰€è¼¸å‡ºçš„çµæœ:\n\næ¨¡å‹ 1 ({model_options[model_1_id]}):\n{model_1_text}\n\næ¨¡å‹ 2 ({model_options[model_2_id]}):\n{model_2_text}\n\nä¸¦è§£é‡‹è©•åˆ†çš„ç†ç”±ï¼Œä»¥åŠå“ªä¸€å€‹æ¨¡å‹çš„è¡¨ç¾æ¯”è¼ƒå¥½ã€‚"
    evaluation_prompt_list = [evaluation_task]
    evaluation_output_placeholder = st.empty()  # Create placeholder for streaming output
    evaluation_text = ""
    for chunk in model_3.stream(input=evaluation_prompt_list):
        evaluation_text += chunk  # Accumulate chunk text
        evaluation_output_placeholder.markdown(f'<div class="output-box">{evaluation_text}</div>', unsafe_allow_html=True)

st.markdown("</div>", unsafe_allow_html=True)
